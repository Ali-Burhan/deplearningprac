{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lower Casing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([['No problem. Here is a sample paragraph of 150 words with HTML tags and punctuation marks:<p>As technology keeps advancing at a rapid pace, the world is becoming more digitalized. It‚Äôs no surprise that the demand for web developers and designers has increased significantly in recent years. HTML remains the backbone of the web, and it is essential to learn it as it forms the foundation of all web development. With basic knowledge of HTML tags such as headings, paragraphs, images, and links, you can design and develop simple web pages. However, as your web development skills advance, you will need to master complex tags like forms and tables to create more interactive and user-friendly websites. The great thing about HTML is that it is a straightforward language, and there are lots of online resources that can help you learn it in no time.</p><p>Moreover, HTML is compatible with other web technologies like CSS and JavaScript, making it possible to style and add interactivity to web pages. CSS is responsible for web page styles like colors, typography, and layout, while JavaScript adds functionality. HTML, CSS, and JavaScript make up the three main web technologies and are critical to web development. Mastering these technologies will allow you to build dynamic, responsive, and modern websites that meet the demands of the digital age. In conclusion, learning HTML is the first step to becoming a web developer, and with practice and dedication, you can achieve great things in your web development journey.</p>',1]],columns=['review','sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review']=dataset['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing HTML Tags**\n",
    "- using Reglar Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tag(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    no problem. here is a sample paragraph of 150 ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review'].apply(remove_html_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove URLs from dataset**\n",
    "- Again use Regular Expressions to remove urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Punctuations From Dataset**\n",
    "- using string library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, \"\")\n",
    "    return text\n",
    "\n",
    "# this takes a long time for big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    no problem here is a sample paragraph of 150 w...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review'].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat Word Treatment**\n",
    "- Some words used like abbrevation \n",
    "- ASAP\n",
    "- GN\n",
    "- LMAO\n",
    "- TBH\n",
    "- IDK\n",
    "- change them to their full form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collects all the chat words from git repo sms slang translator \n",
    "chat_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spelling Correction**\n",
    "- use texblob library for spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "incorrect_txt = 'ceertain huw ar yu'\n",
    "\n",
    "textBlb = TextBlob(incorrect_txt)\n",
    "\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Stop Words**\n",
    "- Many stop words like (i her of the, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Emojis**\n",
    "- üòäüòçüòò‚úÖüëå‚ô•\n",
    "- remove or replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(txt):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\" #Emotions\n",
    "                                u\"\\U0001F300-\\U0001F5FF\" #Symbols\n",
    "                                u\"\\U0001F680-\\U0001F6FF\" #Transport\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\" #IOS Flags\n",
    "                                u\"\\U00002707-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                \"]+\",flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',txt)\n",
    "\n",
    "# This Code is use to remove all the emojis and special chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to remove emojis is using emoji library\n",
    "import emoji\n",
    "print(emoji.demojize('python is üî•'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "- Two ways\n",
    "1. **Word-level tokenization**: Splitting text into individual words or tokens. Example\n",
    "- Input: \"Hello, how are you?\"\n",
    "- Output: [\"Hello\", \"how\", \"are\", \"you\"]\n",
    "2. **Character-level tokenization**: Splitting text into individual characters. Example\n",
    "- Input: \"Hello\"\n",
    "- Output: [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "**Stopwords**\n",
    "- Common words like \"the\", \"and\", \"a\", etc. that do not carry much\n",
    "meaning in a sentence.\n",
    "- Removing stopwords can help reduce dimensionality and improve model performance.\n",
    "**Stemming and Lemmatization**\n",
    "- **Stemming**: Reducing words to their base form using simple rules. Example\n",
    "- Input: \"running\"\n",
    "- Output: \"run\"\n",
    "- **Lemmatization**: Reducing words to their base form using a dictionary. Example\n",
    "- Input: \"running\"\n",
    "- Output: \"run\"\n",
    "**Named Entity Recognition (NER)**\n",
    "- Identifying named entities in text, such as people, places, organizations, etc.\n",
    "Example\n",
    "- Input: \"Apple is a technology company.\"\n",
    "- Output: [\"Apple\" (organization)]\n",
    "**Part-of-Speech (POS) Tagging**\n",
    "- Identifying the part of speech (noun, verb, adjective, etc.) of each word in\n",
    "a sentence.\n",
    "Example\n",
    "- Input: \"The quick brown fox jumps over the lazy dog.\"\n",
    "- Output: [\"The\" (article), \"quick\" (adjective), \"brown\" (\n",
    "    adjective), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First way is to use split function\n",
    "text = \"hwy we can make a good car. We can give compeletion to bugatti.\"\n",
    "text.split() # This do word toknization\n",
    "text.split('.') # This will do sentence type tokenization\n",
    "# But this split approach do not handle the special character plus \n",
    "# do not handle chat words\n",
    "\n",
    "#############################Regular Expression#############################\n",
    "# Second way is to use RE\n",
    "import re\n",
    "token = re.findall(\"[\\w']+\",text)\n",
    "# This will handle the special character and chat words\n",
    "# This can handle dot, ? and many other special chars\n",
    "\n",
    "##################################NLTK####################################\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "word_tokenize(text)\n",
    "sent_tokenize(text)\n",
    "\n",
    "\n",
    "#################################Spacy####################################\n",
    "import spacy\n",
    "npl = spacy.load('en_core_web_sm')\n",
    "result = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################Stemming######################################\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(txt):\n",
    "  return \" \".join([ps.stem(word) for word in txt.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################Lemmatizing########################################3\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in text:\n",
    "    print(word_lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
